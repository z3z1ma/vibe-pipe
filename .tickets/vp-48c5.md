---
"id": "vp-48c5"
"status": "open"
"deps": []
"links": []
"created": "2026-01-28T01:27:18Z"
"type": "example"
"priority": 1
"assignee": "z3z1ma"
"tags":
- "phase2"
- "examples"
- "etl"
"external": {}
---
# Real-World Example - ETL Pipeline

Build comprehensive ETL pipeline example demonstrating database and file connectors.

## Tasks
1. Create end-to-end ETL pipeline: PostgreSQL → Parquet → Dashboard
2. Include data quality checks
3. Include error handling and retry logic
4. Include incremental loading
5. Include scheduling example
6. Document the example with README
7. Add to CLI project templates
8. Create integration tests

## Pipeline Flow:
1. Read from PostgreSQL (customers table)
2. Transform and validate data
3. Write to Parquet files (partitioned by date)
4. Generate quality report
5. Load to analytics database

## Example Structure:
```bash
examples/etl_pipeline/
├── README.md
├── vibepiper.toml
├── pipeline.py
├── schemas.py
├── docker-compose.yml
└── tests/
    └── test_pipeline.py
```

## Dependencies
- vp-201 (Database connectors)
- vp-202 (File I/O)
- vp-206 (CLI)

## Acceptance Criteria:
ETL pipeline working end-to-end
Uses database connector (PostgreSQL)
Uses file connector (Parquet)
Has validation checks
Has error handling
Documented with comments
Included in CLI template
Integration tests passing
