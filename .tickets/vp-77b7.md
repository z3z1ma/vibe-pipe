---
"id": "vp-77b7"
"status": "open"
"deps": []
"links": []
"created": "2026-01-28T01:26:23Z"
"type": "feature"
"priority": 1
"assignee": "z3z1ma"
"tags":
- "phase2"
- "integration"
- "files"
"external": {}
---
# File I/O Abstractions

Build file I/O abstractions for common formats (CSV, JSON, Parquet, Excel) to enable reading and writing data files.

## Tasks
1. Create FileReader and FileWriter protocols
2. Implement CSV reader/writer (with dialect support)
3. Implement JSON reader/writer (with schema validation)
4. Implement Parquet reader/writer (with compression)
5. Implement Excel reader/writer (multi-sheet support)
6. Add schema inference from files
7. Add compression support (gzip, zip, snappy)
8. Implement type mapping (file types → VibePiper types)

## Example Usage
```python
from vibe_piper import asset
from vibe_piper.connectors.files import CSVReader

@asset
def customers_csv():
    reader = CSVReader("data/customers.csv")
    return reader.read(schema=CustomerSchema)

@asset
def output_parquet(customers):
    writer = ParquetWriter("output/customers.parquet")
    return writer.write(customers, compression="snappy")
```

## Dependencies
- vp-101 (IO Manager abstraction)

## Technical Notes
- Use pandas for CSV and Excel
- Use pyarrow or fastparquet for Parquet
- Auto-detect delimiters for CSV
- Handle large files with chunking
- Support partitioned file writing

## Acceptance Criteria

4 file formats supported (CSV, JSON, Parquet, Excel)
Schema inference works from file structure
Compression supported (gzip, zip, snappy)
Type mapping (file types → VibePiper types)
Tests with sample data files
Documentation and examples
Test coverage > 85%
